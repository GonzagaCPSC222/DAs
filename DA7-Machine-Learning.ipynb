{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 222](https://github.com/GonzagaCPSC222) Intro to Data Science\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "## DA7 Machine Learning (100 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Trace the kNN algorithm on a dataset\n",
    "* Utilize a kNN classifier and a decision tree classifier from Sci-kit Learn\n",
    "* Utilize a linear regressor and a decision tree regressor from Sci-kit Learn\n",
    "* Evaluate models using train/test sets sampled using the hold out method\n",
    " \n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Use SciPy to perform hypothesis testing using t-tests\n",
    "* Use Jupyter Notebook to tell a data science story\n",
    "* Use Pandas for data analysis\n",
    "* Use Matplotlib to visualize data\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Kaggle's [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/)\n",
    "* Kaggle's [Ames Housing Dataset](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repositories to track code changes and submit your assignment. Open this DA7 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/flCnpTct\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC222/da7-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up. We will grade your most recent commit, even if that commit is after the due date (your work will be marked late if this is the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn Titanic Classification (35 pts)\n",
    "In a Jupyter Notebook called TitanicClassification.ipynb, we are going to tell a story about exploratory data analysis (EDA) and classification of a \"classic\" machine learning dataset, the [Titanic survival dataset](https://www.kaggle.com/c/titanic). Here is a description of the features and the class (\"Survived\"):\n",
    "\n",
    "Variable|Definition|Key\n",
    "-|-|-\n",
    "Survived|Survival|0 = No, 1 = Yes\n",
    "Pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "Sex|Sex|\n",
    "Age|Age in years|\n",
    "SibSp|# of siblings / spouses aboard the Titanic|\n",
    "Parch|# of parents / children aboard the Titanic|\n",
    "Ticket|Ticket number|\n",
    "Fare|Passenger fare|\n",
    "Cabin|Cabin number|\n",
    "Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "### Collect/Clean/Prepare Data\n",
    "1. Download the titanic.csv file from the DAs repo on Github: https://github.com/GonzagaCPSC222/DAs/blob/master/files.\n",
    "1. Load the data, setting the \"PassengerId\" column to be the DataFrame index\n",
    "1. Clean the data\n",
    "    1. Drop columns that are not informative (e.g. at least ones that are unique to each passenger)\n",
    "    1. Convert remaining categorical feature(s) to be numeric. Use [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to do this\n",
    "    1. Drop rows that have a missing value\n",
    "    \n",
    "### Exploratory Data Analysis\n",
    "If you've seen the [Titanic movie](https://en.wikipedia.org/wiki/Titanic_(1997_film), you likely already have a few hunches (or more scientifically, hypotheses) about which of these features are predictive of survival. If you haven't seen the movie, no worries! EDA will help you develop a few hypotheses of your own.\n",
    "1. Group the data by \"Survived\" and use [`get_group()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html) and [`describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) to provide some summary stats/discussion points for those that survived and didn't survive.\n",
    "1. Create at least two visualizations of the data to explore what features are or are not related to survivability.\n",
    "1. Apply t-tests to test at least one of your hypotheses about which features can be used to discriminate between those who survived and didn't survive.\n",
    "1. Store the \"Survived\" class labels in a separate 1D data structure (e.g. separate the y from the X)\n",
    "1. Train a [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on the entire dataset. Set the `max_depth` of the tree to be 3 so your decision tree isn't really tall. \n",
    "    1. Create a visualization of the tree using [sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). Set the appropriate keyword arguments so you can look at your tree and easily interpret the rules. For example, `plot_tree(clf, feature_names=X.columns, class_names={1: \"survived\", 0: \"died\"}, filled=True)`\n",
    "    1. Decision trees are great for EDA because they naturally do a process called \"feature selection.\" That is, the more informative/discriminatory features will be towards the top of the tree. How do your hypotheses compare to your tree output?\n",
    "    \n",
    "### Build/Refine Machine Learning Models\n",
    "1. Scale the features using [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "1. Evaluate the accuracy of a [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifier using the hold out method for dividing the dataset in to training and testing sets\n",
    "    1. Use a `random_state` of 0 for reproducibility\n",
    "    1. Reserve 25% of the dataset for testing\n",
    "    1. Explore improving the classifier by trying different values for the number of neighbors k. What is the best accuracy you can achieve?\n",
    "1. Using the same train/test splits, compare your KNeighborsClassifier results to results from a [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (10 pts)\n",
    "Participate in the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) Kaggle Competition. Follow the directions and submit your predictions for their provided test set using your \"best\" classifier from the Titanic exercises above. Note that your actual accuracy for this isn't what is important. What is important is just trying it out and having fun with it ðŸ˜€\n",
    "\n",
    "Include your code to produce the predictions, the CSV file you uploaded for your predictions, and a screenshot of submission confirmation (with the accuracy shown) on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends Regression Exercise (15 pts)\n",
    "In a Jupyter Notebook called GoogleTrendsRegression.ipynb, we are going to use the least squares linear regression algorithm to fit a line. For the dataset, we are going to download data from Google Trends. From [Wikipedia](https://en.wikipedia.org/wiki/Google_Trends):\n",
    ">Google Trends is a website by Google that analyzes the popularity of top search queries in Google Search across various regions and languages. The website uses graphs to compare the search volume of different queries over time.\n",
    "\n",
    "Go to https://trends.google.com/trends and play around a bit (there are some fun starter trends on the homepage):\n",
    "\n",
    "<img src=\"https://github.com/GonzagaCPSC222/DAs/raw/master/figures/google_trends.png\" width=\"400\">\n",
    "\n",
    "After you have a sense for how Google Trends works, explore search terms/topics that you think are correlated with \"Gonzaga University.\" Make sure these search terms/topics do not have \"Gonzaga\" or \"University\" in them. \n",
    "\n",
    "Download the \"Interest over time\" data for the past 5 years for one of your search terms/topics compared with \"Gonzaga University.\" Perform data cleaning and preparation on this dataset so you can fit a line to it. Our \"x\" values will be the weekly search popularity for your search term/topic and our \"y\" values will be the corresponding weekly search popularity for \"Gonzaga University.\" Fit a line y = mx + b to the data. Print the resulting linear equation and its correlation coefficient. Display a scatter chart of the x,y data with the linear regression line. How well does your search/term topic predict \"Gonzaga University\" popularity? I challenge you to find a search/term topic with a correlation to \"Gonzaga University\" of at least 0.5 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn Housing Regression (25 pts)\n",
    "In a Jupyter Notebook called HousingRegression.ipynb, we are going to tell a story about exploratory data analysis (EDA) and regression of house prices dataset, Kaggle's [Ames Housing Dataset](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset). \n",
    "\n",
    "### Collect/Clean/Prepare Data\n",
    "1. Download AmesHousing.csv from the DAs repo on Github: https://github.com/GonzagaCPSC222/DAs/blob/master/files\n",
    "1. Read through the file and the [descriptions here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt) to get a sense of the available features\n",
    "    1. Note that \"SalePrice\" is the target `y` variable we want to predict\n",
    "1. Load the data, setting the \"PID\" column to be the DataFrame index\n",
    "1. Clean the data\n",
    "    1. Drop columns that have at least 25% missing data\n",
    "    1. Drop rows that have a missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "For EDA for this regression problem, choose at a subset of the features (at least 5, though you'll probably want to explore different subsets and sizes) that you think are predictive of \"SalePrice\". Choose at least one categorical feature so you can get practice converting categorical attribute to numeric attributes. Use [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to do this (its interface is similar to `MinMaxScaler`). \n",
    "\n",
    "After you've trimmed your dataframe to \"SalePrice\" and the features you think will be predictive of \"SalePrice\", produce a correlation matrix heatmap using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()\n",
    "corr_df.style.background_gradient(cmap='bwr').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are highly correlated with our target variable? Which features are not correlated with our target variable? Feel free to explore different feature subsets (or all of them, though that will be alot!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build/Refine Machine Learning Models\n",
    "1. Evaluate the coefficient of determination $R^2$ of a [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) regressor using the hold-out method for dividing the dataset in to training and testing sets\n",
    "    1. Use a `random_state` of 0 for reproducibility\n",
    "    1. Reserve 25% of the dataset for testing\n",
    "    1. Plot the predicted values against the actual values in a scatter chart\n",
    "    1. Explore improving the regressor by removing features based on their correlation with the target variable. What is the best $R^2$ you can achieve?\n",
    "1. Using the same train/test splits, compare your LinearRegression results to results from a [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ethics (15 pts)\n",
    "Watch Netflix's documentary, [\"The Social Dilemma\"](https://www.netflix.com/title/81254224). While I encourage you to watch the whole documentary (1 hour and 34 mins), I'm only assigning the following ~45 minute chunk: 5:45 (title page) to 49:01. \n",
    "\n",
    "Note: If you do not have access to Netflix, you can read the transcript of the documentary [here](https://scrapsfromtheloft.com/2020/10/03/the-social-dilemma-movie-transcript/). Read from \"Aza does welcoming remarks. We play the video.\" to \"Because theyâ€™re controlling, you know, the information that we see, theyâ€™re controlling us more than weâ€™re controlling them.\" (this part of the transcript corresponds with the assigned timestamps above). Use the web browser search feature (e.g. ctrl + F or cmd + F) to search for the start of the reading.\n",
    "\n",
    "Please watch/read the assigned chunk directly and resist the urge to use AI to summarize it. Also, while I don't mind if you use AI to work with you to brainstorm ideas, **please do not outsource your thinking and writing to AI**. If you do use AI in any capacity, you must cite your use of it.\n",
    "\n",
    "In a Jupyter Notebook called Ethics.ipynb, provide your reflection on the following discussion points:\n",
    "1. Toward the beginning of the documentary, viewers are presented with the idea that social media sells our attention to advertisers to make their profits. The quote that best summarizes this business model is, \"If youâ€™re not paying for the product, then you are the product.\" How do you feel about selling your attention to use the products for free? Are there short and long term effects that we can/cannot foresee?\n",
    "1. Tristan Harris states, \"On the other side of the screen, itâ€™s almost as if they had this avatar voodoo doll-like model of us. All of the things we've ever done, all the clicks weâ€™ve ever made, all the videos weâ€™ve watched, all the likes, that all gets brought back into building a more and more accurate model. The model, once you have it, you can predict the kinds of things that person does.\" He then states that tech companies have three main goals: engagement (drive up your usage), growth (keep you coming back and inviting friends), and advertising (making as much money as possible from advertising). Are there ethical lines that companies should not cross in order to achieve these three goals? Do you feel that the \"avatar voodoo doll-like model of us\" crosses these lines? Why or why not?\n",
    "1. How do you feel about persuasive technology (e.g. programming of the \"the positive intermittent reinforcement\", AKA the slot machine)? As humans, are we able to differentiate between our original thoughts and ones that are \"planted\" unconsciously by persuasive technology?\n",
    "1. What else struck you about this documentary?\n",
    "\n",
    "Your reflection write-up should be written using full sentences and should be grammatically correct. Proof read your writing before you submit it!!\n",
    "\n",
    "(optional) Lastly, if you are interested in some light reading over break, I posted 3 additional chapters (and the conclusion) from [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815) by Cathy O'Neil in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Turn in your assignment files via a Github Classroom repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this.\n",
    "    1. Your repo should contain all of the files needed to run and test your solution (e.g. .py file(s), input files, etc.).\n",
    "    1. Double-check that this is the case by \"pretending to be the grader\": clone (or download a zip) your submission repo and run your code like when we grade it.\n",
    "1. Submit this DAâ€™s associated assignment in Canvas to mark your DA as \"done\" and ready for grading. We will then pull your Github repo and grade your DA as soon as possible. The date and time you submit the DA assignment in Canvas will be used for marking your assignment as \"late\" or \"on-time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 100 points + 10 points bonus. Your assignment will be evaluated based on a successful execution in Jupyter Lab (using the Anaconda Python Distribution v3.12) and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 35 pts for Titanic classification\n",
    "    * 5 pts for cleaning the data\n",
    "    * 5 pts for at least two visualizations\n",
    "    * 5 pts for at least one hypothesis test\n",
    "    * 5 pts for plotting a decision tree\n",
    "    * 10 pts for evaluating and attempting to improve upon the accuracy of a kNN classifer\n",
    "    * 5 pts for comparing the accuracies of a kNN classifer with a decision tree classifier\n",
    "* 15 pts for fitting a line to Google Trends data\n",
    "* 25 pts for house prices regression\n",
    "    * 5 pts for cleaning the data\n",
    "    * 5 pts for producing and interpreting the correlation matrix heatmap\n",
    "    * 10 pts for evaluating and attempting to improve upon the accuracy of a linear regression model\n",
    "    * 5 pts for comparing the coefficients of determination of a linear regression model with a decision tree regressor\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC222/DAs/blob/master/Coding%20Standard.ipynb)\n",
    "    * This includes writing Jupyter Notebooks that tell stories with well-organized, interleaved code cells and markdown cells\n",
    "* 15 pts for quality, clarity, and creativity in the data ethics write-up"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
